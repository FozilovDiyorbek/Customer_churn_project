{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "49358220",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score, classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "import joblib\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "006184db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (12000, 10)\n",
      "Test shape: (3000, 10)\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "df = pd.read_csv(\"C:/Projects/Customer_churn_project/data/processed/cleaned_customer_churn.csv\")\n",
    "X = df.drop(\"Exited\", axis=1)\n",
    "y = df[\"Exited\"]\n",
    "\n",
    "# Train/Test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "with open(\"C:/Projects/Customer_churn_project/models/preprocessing_pipeline.pkl\", \"rb\") as f:\n",
    "    preprocessing_pipeline = pickle.load(f)\n",
    "\n",
    "print(\"Train shape:\", X_train.shape)\n",
    "print(\"Test shape:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f023b316",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{np.float64(0.0): np.float64(0.6238303181534622), np.float64(1.0): np.float64(2.5188916876574305)}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import numpy as np\n",
    "\n",
    "classes = np.unique(y_train)\n",
    "weights = compute_class_weight(class_weight='balanced', classes=classes, y=y_train)\n",
    "weights_dict = dict(zip(classes, weights))\n",
    "print(weights_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f31bcc86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy: 0.8243\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.95      0.83      0.88      2405\n",
      "         1.0       0.54      0.81      0.65       595\n",
      "\n",
      "    accuracy                           0.82      3000\n",
      "   macro avg       0.74      0.82      0.76      3000\n",
      "weighted avg       0.86      0.82      0.84      3000\n",
      "\n",
      "Random Forest Accuracy: 0.8943\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.92      0.96      0.94      2405\n",
      "         1.0       0.78      0.64      0.71       595\n",
      "\n",
      "    accuracy                           0.89      3000\n",
      "   macro avg       0.85      0.80      0.82      3000\n",
      "weighted avg       0.89      0.89      0.89      3000\n",
      "\n",
      "XGBoost Accuracy: 0.8817\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.94      0.91      0.92      2405\n",
      "         1.0       0.67      0.79      0.73       595\n",
      "\n",
      "    accuracy                           0.88      3000\n",
      "   macro avg       0.81      0.85      0.82      3000\n",
      "weighted avg       0.89      0.88      0.89      3000\n",
      "\n",
      "[LightGBM] [Info] Number of positive: 2382, number of negative: 9618\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001853 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 847\n",
      "[LightGBM] [Info] Number of data points in the train set: 12000, number of used features: 13\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.198500 -> initscore=-1.395696\n",
      "[LightGBM] [Info] Start training from score -1.395696\n",
      "LightGBM Accuracy: 0.8690\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.96      0.88      0.91      2405\n",
      "         1.0       0.63      0.84      0.72       595\n",
      "\n",
      "    accuracy                           0.87      3000\n",
      "   macro avg       0.79      0.86      0.82      3000\n",
      "weighted avg       0.89      0.87      0.88      3000\n",
      "\n",
      "Gradient Boosting Accuracy: 0.9000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.92      0.96      0.94      2405\n",
      "         1.0       0.79      0.67      0.73       595\n",
      "\n",
      "    accuracy                           0.90      3000\n",
      "   macro avg       0.86      0.81      0.83      3000\n",
      "weighted avg       0.90      0.90      0.90      3000\n",
      "\n",
      "CatBoost Accuracy: 0.8683\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.95      0.88      0.91      2405\n",
      "         1.0       0.63      0.83      0.71       595\n",
      "\n",
      "    accuracy                           0.87      3000\n",
      "   macro avg       0.79      0.85      0.81      3000\n",
      "weighted avg       0.89      0.87      0.87      3000\n",
      "\n",
      "\n",
      " Eng yaxshi model: Gradient Boosting (Accuracy: 0.9000)\n"
     ]
    }
   ],
   "source": [
    "class_weights = {0: 0.624, 1: 2.519}\n",
    "\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(\n",
    "        max_iter=1000,\n",
    "        class_weight=class_weights\n",
    "    ),\n",
    "\n",
    "    \"Random Forest\": RandomForestClassifier(\n",
    "        n_estimators=200,\n",
    "        random_state=42,\n",
    "        class_weight=class_weights\n",
    "    ),\n",
    "\n",
    "    \"XGBoost\": XGBClassifier(\n",
    "        eval_metric='logloss',\n",
    "        use_label_encoder=False,\n",
    "        random_state=42,\n",
    "        scale_pos_weight=class_weights[1] / class_weights[0]  \n",
    "    ),\n",
    "\n",
    "    \"LightGBM\": LGBMClassifier(\n",
    "        random_state=42,\n",
    "        scale_pos_weight=class_weights[1] / class_weights[0]\n",
    "    ),\n",
    "\n",
    "    \"Gradient Boosting\": GradientBoostingClassifier(\n",
    "        random_state=42\n",
    "    ),\n",
    "\n",
    "    \"CatBoost\": CatBoostClassifier(\n",
    "        verbose=0,\n",
    "        random_state=42,\n",
    "        class_weights=[class_weights[0], class_weights[1]]\n",
    "    )\n",
    "}\n",
    "\n",
    "best_model = None\n",
    "best_score = 0\n",
    "best_model_name = None\n",
    "\n",
    "for name, model in models.items():\n",
    "    pipeline = ImbPipeline([\n",
    "        (\"preprocess\", preprocessing_pipeline),\n",
    "        #(\"smote\", SMOTE(random_state=42)),  \n",
    "        (\"model\", model)\n",
    "    ])\n",
    "    \n",
    "    pipeline.fit(X_train, y_train)\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "    \n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    print(f\"{name} Accuracy: {acc:.4f}\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    if acc > best_score:\n",
    "        best_score = acc\n",
    "        best_model = pipeline\n",
    "        best_model_name = name\n",
    "\n",
    "print(f\"\\n Eng yaxshi model: {best_model_name} (Accuracy: {best_score:.4f})\")\n",
    "\n",
    "\n",
    "with open(\"C:/Projects/Customer_churn_project/models/best_model.pkl\", \"wb\") as f:\n",
    "    pickle.dump(best_model, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
